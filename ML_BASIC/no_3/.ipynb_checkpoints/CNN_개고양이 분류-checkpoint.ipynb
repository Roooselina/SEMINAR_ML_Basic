{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce03de7-3221-4011-9772-a17726fe3726",
   "metadata": {},
   "source": [
    "## 이번 머신러닝 학습 계획\n",
    "\n",
    "개, 고양이 이미지를 각각 1000개씩 가지고 있고, 테스트 이미지도 1000개씩 가지고 있다.\n",
    "\n",
    "강아지 레이블을 0, 고양이 레이블을 1로 하여 이미지를 저장한다.\n",
    "\n",
    "**이번 코드를 짜며 가장 이루고 싶은 목표는 torch, data_loader을 쓰지 않고 리스트로 해결하는 것이다.**\n",
    "\n",
    "**가능하다면 위 목표를 지키기 위해 노력하겠지만, 정 어렵다면 최소한으로 사용하는 것을 목표로 한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21fe2220-14d1-4ae0-ab56-7bb8fc56f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "torch.backends.cuda.max_split_size_mb = 0\n",
    "torch.backends.cuda.max_sync_interval = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d6a821-5711-440e-a054-52ae8da66c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2809fcd5-7327-4f5d-8b90-3f9948bd4f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "##현재 데이터는 각 폴더에 1000개씩 저장되어 있다.\n",
    "##이를 label, 이미지로 저장해보도록 하겠다.\n",
    "\n",
    "\n",
    "cat_path = 'C:\\\\Users\\\\kuroc\\\\Documents\\\\no_3\\\\02_cnn_pt\\\\train\\\\cat'\n",
    "dog_path = 'C:\\\\Users\\\\kuroc\\\\Documents\\\\no_3\\\\02_cnn_pt\\\\train\\\\dog'\n",
    "\n",
    "# 데이터셋 초기화; 이미지:라벨로 저장\n",
    "cat_data = {'images': [], 'labels': []}\n",
    "dog_data = {'images': [], 'labels': []}\n",
    "\n",
    "# 디렉토리 내의 모든 파일 목록 얻기\n",
    "cfile_list = os.listdir(cat_path)\n",
    "dfile_list = os.listdir(dog_path)  # 수정 필요\n",
    "\n",
    "print(len(cfile_list))\n",
    "print(len(dfile_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7c17cf-c3c9-4f21-b0f3-4c55756c7c99",
   "metadata": {},
   "source": [
    "## 고양이 먼저 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5764c63-3514-4290-b85d-f8f2d274334c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 디렉토리 내의 각 이미지에 대해 반복\n",
    "'''강아지를 0, 고양이를 1로 하여 저장할 것이다.'''\n",
    "\n",
    "for file_name in cfile_list:\n",
    "    img_path = os.path.join(cat_path, file_name)\n",
    "    img_array=np.fromfile(img_path, np.uint8)\n",
    "    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "    # 이미지를 읽어오기\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # 이미지 파일 이름에서 숫자 정보 추출\n",
    "    '''\n",
    "    이미지 형식은 \"해당 숫자.순번\"형식이다.\n",
    "    따라서 .를 기준으로 숫자를 분리한다.\n",
    "    '''\n",
    "    label = 1\n",
    "\n",
    "    # 이미지와 레이블을 데이터셋에 추가\n",
    "    cat_data['images'].append(img)\n",
    "    cat_data['labels'].append(label)\n",
    "\n",
    "print(len(cat_data['images']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fd2c4-964e-48fa-9029-d3bf684b8b53",
   "metadata": {},
   "source": [
    "## 강아지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21a412e6-f621-4526-9280-e2ca8ff57515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# 디렉토리 내의 각 이미지에 대해 반복\n",
    "'''강아지를 0, 고양이를 1로 하여 저장할 것이다.'''\n",
    "\n",
    "for file_name in dfile_list:\n",
    "    img_path = os.path.join(dog_path, file_name)\n",
    "    img_array=np.fromfile(img_path, np.uint8)\n",
    "    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "    # 이미지를 읽어오기\n",
    "\n",
    "    # 이미지 파일 이름에서 숫자 정보 추출\n",
    "    '''\n",
    "    이미지 형식은 \"해당 숫자.순번\"형식이다.\n",
    "    따라서 .를 기준으로 숫자를 분리한다.\n",
    "    '''\n",
    "    label = 0\n",
    "\n",
    "    # 이미지와 레이블을 데이터셋에 추가\n",
    "    dog_data['images'].append(img)\n",
    "    dog_data['labels'].append(label)\n",
    "\n",
    "print(len(dog_data['images']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e1f2a4-117f-437d-95aa-4dd15692ad2a",
   "metadata": {},
   "source": [
    "## 두 데이터를 합쳐서 하나의 데이터로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e785bb-c46d-4d43-b3c5-f02893466561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "data = {'images': cat_data['images'] + dog_data['images'], 'labels': cat_data['labels'] + dog_data['labels']}\n",
    "print(len(data['images']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941a4969-d751-43c3-bd66-007c383c236c",
   "metadata": {},
   "source": [
    "## 이미지 전처리\n",
    "CNN에서는 fully-connected network 와 마찬가지로 고정적인 input이 필요하다.\n",
    "\n",
    "그래서 나는 image의 크기를 250, 250으로 맞추어 이미지를 전처리 해보도록 하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef7889a8-4a1e-43f6-a537-4278b10a00ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.5682431e-03 2.5682431e-03 2.5836218e-03 ... 3.1372549e-03\n",
      "  3.1218762e-03 3.1064975e-03]\n",
      " [2.5682431e-03 2.5682431e-03 2.5836218e-03 ... 3.1526336e-03\n",
      "  3.1372549e-03 3.1064975e-03]\n",
      " [2.5682431e-03 2.5682431e-03 2.5836218e-03 ... 3.1526336e-03\n",
      "  3.1372549e-03 3.1218762e-03]\n",
      " ...\n",
      " [1.9069589e-03 1.9069589e-03 1.9069589e-03 ... 3.0757401e-05\n",
      "  3.0757401e-05 3.0757401e-05]\n",
      " [1.8915802e-03 1.8915802e-03 1.8915802e-03 ... 3.0757401e-05\n",
      "  3.0757401e-05 3.0757401e-05]\n",
      " [1.8762015e-03 1.8762015e-03 1.8762015e-03 ... 1.5378700e-05\n",
      "  1.5378700e-05 1.5378700e-05]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "유의할 점: data={'images':'labels'}형이므로, len(data)라고 하면 2개 값만 처리 된다.\n",
    "키 값을 len값으로 처리하는 게 좋다.'''\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(data['images'])):\n",
    "    data['images'][i] = (data['images'][i] / 255.0).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(data['images'][0]) ##제대로 0~1로 정의되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba3a28-a21d-46be-b39d-099b623dfefb",
   "metadata": {},
   "source": [
    "## CNN 클래스 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22e05ba6-8956-4a7a-8858-0327cd7f7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=632, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Conv2d(632, 800, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Conv2d(800, 900, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.linear3 = nn.Sequential(\n",
    "            nn.Conv2d(900, 1000, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(225000, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000, 2, bias=True)\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.pool(out)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.linear3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = CNN()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2185142-2621-438f-b389-202dc18c17cf",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "\n",
    "pytorch.dataloader 기능을 쓸 수가 없어서 이를 그대로 구현해주기로 했다.\n",
    "\n",
    "우선은 batch 기능을 먼저 구현해보고, 추후에 시간이 되면 셔플 기능을 추가해보기로 한다.\n",
    "\n",
    "dict1, dict2..식으로 저장하기에는 너무 개수가 많으므로 리스트에 모든 딕셔너리를 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6336af1-c975-4d26-a29d-25a2143c0ef2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 3), <f4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3070\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3069\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3070\u001b[0m     mode, rawmode \u001b[38;5;241m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[0;32m   3071\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: ((1, 1, 3), '<f4')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m current_batch \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m---> 17\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)  \u001b[38;5;66;03m# numpy 배열을 이미지로 변환\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     img \u001b[38;5;241m=\u001b[39m resize_image(img, (\u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m250\u001b[39m))\n\u001b[0;32m     19\u001b[0m     current_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(img)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3073\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3071\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   3072\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m typekey\n\u001b[1;32m-> 3073\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   3074\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3075\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 3), <f4"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "# 빈 리스트(딕셔너리 저장할 리스트).\n",
    "batched_dicts = []\n",
    "\n",
    "def resize_image(img, target_size):\n",
    "    transform = transforms.Compose([transforms.Resize(target_size),\n",
    "                                    transforms.ToTensor()])\n",
    "    return transform(img)\n",
    "\n",
    "\n",
    "# 딕셔너리의 아이템을 순회하면서 20개만큼 값 추가.\n",
    "current_batch = {'images': [], 'labels': []}\n",
    "for img, label in zip(data['images'], data['labels']):\n",
    "    img = Image.fromarray(img)  # numpy 배열을 이미지로 변환\n",
    "    img = resize_image(img, (250, 250))\n",
    "    current_batch['images'].append(img)\n",
    "    current_batch['labels'].append(label)\n",
    "    if len(current_batch['images']) == batch_size:\n",
    "        batched_dicts.append(current_batch)\n",
    "        current_batch = {'images': [], 'labels': []}\n",
    "\n",
    "print(len(batched_dicts))\n",
    "print(len(batched_dicts[0]))\n",
    "print(batched_dicts[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05eca5-bbf2-4d89-896f-07d2e72a7103",
   "metadata": {},
   "source": [
    "CNN 학습을 위해선 conv2d 를 써야 하는데, 입력값이 무조건 tensor여야만 사용할 수 있다.\n",
    "\n",
    "그래서 여기서 x를 tensor화 해주기로 했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4557e5-73e3-4483-8729-5b2006398605",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(batched_dicts)\n",
    "\n",
    "epoch = 20\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for e in range(epoch):\n",
    "    total_cost = 0  # 새로운 변수로 변경\n",
    "    for batch in batched_dicts:\n",
    "        x = torch.stack(batch['images'])  # 이미지 데이터를 텐서로 변환하고 GPU로 이동\n",
    "\n",
    "        y = torch.tensor(batch['labels'])\n",
    "        print(0)\n",
    "        optimizer.zero_grad()\n",
    "        hy = model(x)\n",
    "        cost = criterion(hy, y)\n",
    "        print(2)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        print(1)\n",
    "        total_cost += cost.item()\n",
    "    print(\"cost[{}] :{}\".format(e + 1, total_cost / total))\n",
    "\n",
    "print(\"학습종료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceae6e2c-18a4-4541-9b98-3015fc24c901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
