{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b43adb5-f463-47eb-831f-639f7430aa8e",
   "metadata": {},
   "source": [
    "## 데이터 저장법\n",
    "\n",
    "1. 이미지를 숫자로 나타내기(EX. A=1, B=2...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffa8723-44a7-4f3f-a0b3-9657e7e0ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[0,1,2,3,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde0ae1-88a2-42ab-8cbb-4576eb3ca4da",
   "metadata": {},
   "source": [
    "2. one-hot encoding(내가 표현하고자 하는 값을 벡터로 표현함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ae825b-d099-4f5e-98ea-f33ce3237843",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hell0를 표현하고자 할 때\n",
    "x_hot = [[1,0,0,0,0],\n",
    "         [0,1,0,0,0],\n",
    "         [1,0,0,0,0],\n",
    "         [0,0,1,0,0],\n",
    "         [0,0,0,1,0],\n",
    "         [0,0,0,1,0]]\n",
    "\n",
    "y_data = [[1,0,2,3,3,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36787eb6-7b2f-41f7-8950-a72c345d28d3",
   "metadata": {},
   "source": [
    "##  Cross Entropy Loss\n",
    "\n",
    "다음 단어를 예측하는 알고리즘의 경우, input 값에 따라 결과 값에서 무엇이 올 확률이 가장 높은지 계산해야 함\n",
    "\n",
    "따라서 cross etropy loss를 사용해 결과값을 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd062f71-460a-404f-a2e6-810164118b48",
   "metadata": {},
   "source": [
    "## 1. one hot 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e10d45d7-e431-4c46-8e80-07c6fb95188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "char = ['h','i','e','l','o']\n",
    "input_size = len(char)\n",
    "hideen_size = len(char) ##값 달라도 상관 x\n",
    "learning_rate=0.1\n",
    "\n",
    "x_data = [[0,1,0,2,3,3]]\n",
    "##o를 예측하는 코드이므로 0을 뺐다.\n",
    "x_hot = [[1,0,0,0,0],\n",
    "         [0,1,0,0,0],\n",
    "         [1,0,0,0,0],\n",
    "         [0,0,1,0,0],\n",
    "         [0,0,0,1,0],\n",
    "         [0,0,0,1,0]]\n",
    "\n",
    "y_data = [[1,0,2,3,3,4]]\n",
    "\n",
    "x=torch.FloatTensor(x_hot)\n",
    "y=torch.FloatTensor(y_data)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c931dfc5-898c-45f6-bf6e-d6bbae15dd2b",
   "metadata": {},
   "source": [
    "## 문자열 시퀀스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e55fc08b-425c-4aa9-8f95-c8c3f45dfafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char detail: ['i', 'o', 'e', 'h', 'l'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "word = \"hihello\"\n",
    "\n",
    "char = list(set(word))\n",
    "print(\"char detail:\",char,'\\n')\n",
    "##set은 순서를 보존하지 않는다. 순서가 이상할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb30d2e-1810-4ea7-919e-b2217554da36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary detail: {'i': 0, 'o': 1, 'e': 2, 'h': 3, 'l': 4} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "char_dic = {c:i for i,c in enumerate(char)}\n",
    "print(\"dictionary detail:\",char_dic,'\\n')\n",
    "## char에 있는 문자에 대해 i로 순서를 매핑한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2180d47-fa52-431f-91d0-51812967b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_size = len(char_dic)\n",
    "hidden_size = len(char_dic)\n",
    "##hideen_size는 변경 가능하다.\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25941fef-07b9-4b15-80ce-b5b5a7c7e223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maked array: [3, 0, 3, 2, 4, 4, 1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_idx = [char_dic[c] for c in word]\n",
    "print(\"maked array:\",word_idx,'\\n')\n",
    "##word에 있는 글자에 따라 char_dic[c] value값을 나열한다.(ex. 3,1,2..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e12347ed-7da4-460f-919a-e831c95904b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.]])]\n"
     ]
    }
   ],
   "source": [
    "x_data = [word_idx[:-1]]\n",
    "##맨 마지막 값만 빼고 전부 하나의 리스트로\n",
    "\n",
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
    "'''\n",
    "one-hot 인코딩을 하는 코드다.\n",
    "np.eye(dic_size) : dic_size 만큼크기의 매트릭스 생성\n",
    "dic_size = 5이다. 즉슨 5x5 배열 생성\n",
    "기본은 대각선이 1이고, 나머지가 0인 배열이 생성되나 [x]로 1 위치 지정 가능\n",
    "\n",
    "x_data에 해당하는 x 위치에 1을 놓고, 나머지는 0으로 채운다.\n",
    "위 방식을 통해 one-hot encoding을 할 수 있다.\n",
    "'''\n",
    "\n",
    "print(x_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea643ed7-1721-4920-b8bb-11e862f36c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y data: [[0, 3, 2, 4, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "y_data = [word_idx[1:]]\n",
    "print(\"y data:\",y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c773f85a-870a-40cd-904d-4e5ae9e535fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 1.]]])\n",
      "tensor([[0, 3, 2, 4, 4, 1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kuroc\\AppData\\Local\\Temp\\ipykernel_9832\\2614910100.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  x = torch.FloatTensor(x_one_hot)\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor(x_one_hot)\n",
    "y= torch.LongTensor(y_data)\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a41257-54c3-4ff8-96a3-84d639d24466",
   "metadata": {},
   "source": [
    "## 실제 RNN 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f876ba8b-3524-4acc-b2d9-5215e579ef18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ninput size, hiddeb_size: hihell 길이\\no를 예측하는 것이므로 o까지 포함 x\\nbatch_size를 자동으로 입력하기 위해, batch_first를 True로 설정\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "print(input_size, hidden_size)\n",
    "RNN = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "'''\n",
    "input size, hiddeb_size: hihell 길이\n",
    "o를 예측하는 것이므로 o까지 포함 x\n",
    "batch_size를 자동으로 입력하기 위해, batch_first를 True로 설정\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ade4400-4b9d-452e-b732-f7650544df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(RNN.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79271626-88a5-43e2-87a3-fad0db30b554",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0 ]: 1.778258204460144 prediction: [[3 0 3 0 3 3]] true Y: [[0, 3, 2, 4, 4, 1]] prediction str: hihihh\n",
      "[epoch 10 ]: 0.8783188462257385 prediction: [[0 3 2 4 4 4]] true Y: [[0, 3, 2, 4, 4, 1]] prediction str: ihelll\n",
      "[epoch 20 ]: 0.6632985472679138 prediction: [[0 3 2 4 4 4]] true Y: [[0, 3, 2, 4, 4, 1]] prediction str: ihelll\n",
      "[epoch 30 ]: 0.5877516865730286 prediction: [[0 3 2 4 4 4]] true Y: [[0, 3, 2, 4, 4, 1]] prediction str: ihelll\n",
      "[epoch 40 ]: 0.5203302502632141 prediction: [[0 3 2 4 4 1]] true Y: [[0, 3, 2, 4, 4, 1]] prediction str: ihello\n",
      "[epoch 50 ]: 0.4874875247478485 prediction: [[0 3 2 4 4 1]] true Y: [[0, 3, 2, 4, 4, 1]] prediction str: ihello\n",
      "[epoch 60 ]: 0.47347167134284973 prediction: [[0 3 2 4 4 1]] true Y: [[0, 3, 2, 4, 4, 1]] prediction str: ihello\n",
      "[epoch 70 ]: 0.46745744347572327 prediction: [[0 3 2 4 4 1]] true Y: [[0, 3, 2, 4, 4, 1]] prediction str: ihello\n",
      "[epoch 80 ]: 0.46401306986808777 prediction: [[0 3 2 4 4 1]] true Y: [[0, 3, 2, 4, 4, 1]] prediction str: ihello\n",
      "[epoch 90 ]: 0.4617025852203369 prediction: [[0 3 2 4 4 1]] true Y: [[0, 3, 2, 4, 4, 1]] prediction str: ihello\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    ##outputs: 실제로 나오는 H의 값(5x5 텐서로 입력되었으므로 5x5 크기이다)\n",
    "    ##_status는 다시 순환해서 학습하는 값\n",
    "    optimizer.zero_grad()\n",
    "    outputs, _status = RNN(x)\n",
    "    \n",
    "    ##손실 계산\n",
    "    loss = criterion(outputs.view(-1, input_size), y.view(-1))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    ##3차원 입력값 중에서, 가장 큰 값을 찾는다.(axis=2 = 3차원)\n",
    "    result = outputs.data.numpy().argmax(axis=2)\n",
    "\n",
    "    #result는 squeeze를 통해 1차원 배열로 바뀐다.\n",
    "    #join [c]를 통해 char[c]에 해다하는 글자를 하나로 합친 단어로 만든다.\n",
    "    result_s = ''.join([char[c] for c in np.squeeze(result)])\n",
    "    if i%10 ==0:\n",
    "        print(\"[epoch\", i, \"]:\", loss.item(), \"prediction:\", result, \"true Y:\", y_data, \"prediction str:\", result_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfaf934-881c-49ff-acc6-9cb1a03b6897",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "099bd4cb-8222-48fc-b4fc-dbfd5efc9b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 I am so happy t ->  am so happy to\n",
      "1  am so happy to -> am so happy to \n",
      "2 am so happy to  -> m so happy to s\n",
      "3 m so happy to s ->  so happy to st\n",
      "4  so happy to st -> so happy to stu\n",
      "5 so happy to stu -> o happy to stud\n",
      "6 o happy to stud ->  happy to study\n",
      "7  happy to study -> happy to study \n",
      "8 happy to study  -> appy to study R\n",
      "9 appy to study R -> ppy to study RN\n",
      "10 ppy to study RN -> py to study RNN\n",
      "11 py to study RNN -> y to study RNN.\n",
      "12 y to study RNN. ->  to study RNN. \n",
      "13  to study RNN.  -> to study RNN. T\n",
      "14 to study RNN. T -> o study RNN. Th\n",
      "15 o study RNN. Th ->  study RNN. The\n",
      "16  study RNN. The -> study RNN. Ther\n",
      "17 study RNN. Ther -> tudy RNN. There\n",
      "18 tudy RNN. There -> udy RNN. There \n",
      "19 udy RNN. There  -> dy RNN. There a\n",
      "20 dy RNN. There a -> y RNN. There ar\n",
      "21 y RNN. There ar ->  RNN. There are\n",
      "22  RNN. There are -> RNN. There are \n",
      "23 RNN. There are  -> NN. There are s\n",
      "24 NN. There are s -> N. There are so\n",
      "25 N. There are so -> . There are so \n",
      "26 . There are so  ->  There are so m\n",
      "27  There are so m -> There are so ma\n",
      "28 There are so ma -> here are so man\n",
      "29 here are so man -> ere are so many\n",
      "30 ere are so many -> re are so many \n",
      "31 re are so many  -> e are so many t\n",
      "32 e are so many t ->  are so many th\n",
      "33  are so many th -> are so many thi\n",
      "34 are so many thi -> re so many thin\n",
      "35 re so many thin -> e so many thing\n",
      "36 e so many thing ->  so many things\n",
      "37  so many things -> so many things \n",
      "38 so many things  -> o many things t\n",
      "39 o many things t ->  many things to\n",
      "40  many things to -> many things to \n",
      "41 many things to  -> any things to s\n",
      "42 any things to s -> ny things to st\n",
      "43 ny things to st -> y things to stu\n",
      "44 y things to stu ->  things to stud\n",
      "45  things to stud -> things to study\n",
      "46 things to study -> hings to study.\n",
      "47 hings to study. -> ings to study. \n",
      "48 ings to study.  -> ngs to study. I\n",
      "49 ngs to study. I -> gs to study. It\n",
      "50 gs to study. It -> s to study. It'\n",
      "51 s to study. It' ->  to study. It's\n",
      "52  to study. It's -> to study. It's \n",
      "53 to study. It's  -> o study. It's a\n",
      "54 o study. It's a ->  study. It's am\n",
      "55  study. It's am -> study. It's ama\n",
      "56 study. It's ama -> tudy. It's amaz\n",
      "57 tudy. It's amaz -> udy. It's amazi\n",
      "58 udy. It's amazi -> dy. It's amazin\n",
      "59 dy. It's amazin -> y. It's amazing\n",
      "60 y. It's amazing -> . It's amazing.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch (got input: [1403], target: [915])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(x)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# outputs에 softmax를 적용하지 않고 손실을 계산\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     51\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch (got input: [1403], target: [915])"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "sentence = \"I am so happy to study RNN. There are so many things to study. It's amazing.\"\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "sequence_length = 15\n",
    "hidden_size = input_size = sequence_length\n",
    "\n",
    "char_dic = {char: i for i, char in enumerate(set(sentence))}\n",
    "dic_size = len(char_dic)\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1:i + sequence_length + 1]\n",
    "\n",
    "    print(i, x_str, '->', y_str)\n",
    "    x_data.append([char_dic[c] for c in x_str])\n",
    "    y_data.append([char_dic[c] for c in y_str])\n",
    "\n",
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
    "\n",
    "x = torch.FloatTensor(x_one_hot)\n",
    "y = torch.LongTensor(y_data)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, dic_size, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = x[:, -1, :]  # RNN 레이어의 출력 중 마지막 타임 스텝만 사용\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net = Net(dic_size, hidden_size, 2)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(x)\n",
    "\n",
    "    # outputs에 softmax를 적용하지 않고 손실을 계산\n",
    "    loss = criterion(outputs.view(-1), y.view(-1))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    result = outputs.argmax(dim=1)\n",
    "\n",
    "    predict_str = \"\".join([char for char in sentence[i:i+sequence_length]]) + ''.join([char for char in result])\n",
    "    print(i, predict_str, loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f70389-20eb-41ea-a7b7-940e87a13bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
